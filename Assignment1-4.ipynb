{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 01-07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches for Part 1 (Domain Extraction): ['example.com', 'test.org', 'sub.example', 'example.com', 'test.org', 'sub.example']\n",
      "Matches for Part 2 (Date Extraction): ['12/15/2024', '2024-12-16', '01-01-2025', '2025/03/20']\n",
      "Matches for Part 3 (Price Extraction): [('$', '100.99'), ('€', '150'), ('¥', '500'), ('$', '1,000'), ('€', '99.99'), ('¥', '200')]\n",
      "Matches for Part 4 (Hyperlink Extraction): ['https://example.com', 'http://test.org', 'https://sub.example.net']\n",
      "Matches for Part 5 (Spelling Mistakes): ['teh', 'teh', 'recieve', 'occured', 'definately']\n",
      "Matches for Part 6 (Street Address Extraction): [('99', 'or '), ('123', 'Main St'), ('456', 'Elm Avenue'), ('789', 'Oak Rd Suite 100 are available in our directory')]\n",
      "Matches for Part 7 (Hexadecimal Color Codes): ['FFAABB', '000', 'F00']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sample_text = \"\"\"\n",
    "Visit our site at https://www.example.com for more details. You can also reach us at http://test.org or https://sub.example.net for inquiries.\n",
    "\n",
    "The event will take place on 12/15/2024. However, it could be rescheduled to 2024-12-16. We are planning for a backup date, 01-01-2025, or maybe 2025/03/20.\n",
    "\n",
    "We offer products at $100.99, €1500.75, and ¥5000. Another promotion costs just $1,000. You can also get a discount of €99.99 or ¥2000.\n",
    "\n",
    "For more details, check the <a href=\"https://example.com\">Example Website</a> and <a href=\"http://test.org\">Test Website</a>. We also provide external resources at <a href=\"https://sub.example.net\">Sub Example</a>.\n",
    "\n",
    "The issue was that teh cat teh dog and I recieve a lot of mail. The problem occured due to some technical difficulties, which is definately going to be fixed.\n",
    "\n",
    "Addresses like 123 Main St, Apt 4B, 456 Elm Avenue, and 789 Oak Rd Suite 100 are available in our directory.\n",
    "\n",
    "CSS styles include background-color: #FFAABB; text color: #000; and borders set to #F00.\n",
    "\"\"\"\n",
    "\n",
    "part_1 = r\"(?:https?:\\/\\/)?(?:www\\.)?([a-zA-Z0-9-]+\\.[a-zA-Z]{2,})\"  \n",
    "part_2 = r'(\\b(?:\\d{2}[-/\\s]\\d{2}[-/\\s]\\d{4}|\\d{4}[-/\\s]\\d{2}[-/\\s]\\d{2}|\\d{2}[-/\\s]\\d{2}[-/\\s]\\d{4})\\b)'  \n",
    "part_3 = r\"([€$¥])(\\d{1,3}(?:,\\d{3})*(?:\\.\\d{2})?)\"  \n",
    "part_4 = r'<a\\s+(?:[^>]*?\\s+)?href=\"([^\"]*)\"'  \n",
    "part_5 = r\"\\b(teh|recieve|occured|definately)\\b\"  \n",
    "part_6 = r\"(\\d{1,5})\\s([A-Za-z0-9\\s]+(?:\\s(?:Apt\\s?\\d+[A-Za-z]?|Suite\\s?\\d+))?)\"\n",
    "part_7 = r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\"  \n",
    "\n",
    "def extract_matches(regex, text):\n",
    "    result = re.findall(regex, text)\n",
    "    return result\n",
    "\n",
    "matches_part_1 = extract_matches(part_1, sample_text)  \n",
    "matches_part_2 = extract_matches(part_2, sample_text)  \n",
    "matches_part_3 = extract_matches(part_3, sample_text)  \n",
    "matches_part_4 = extract_matches(part_4, sample_text)  \n",
    "matches_part_5 = extract_matches(part_5, sample_text)  \n",
    "matches_part_6 = extract_matches(part_6, sample_text)  \n",
    "matches_part_7 = extract_matches(part_7, sample_text)  \n",
    "\n",
    "print(\"Matches for Part 1 (Domain Extraction):\", matches_part_1)\n",
    "print(\"Matches for Part 2 (Date Extraction):\", matches_part_2)\n",
    "print(\"Matches for Part 3 (Price Extraction):\", matches_part_3)\n",
    "print(\"Matches for Part 4 (Hyperlink Extraction):\", matches_part_4)\n",
    "print(\"Matches for Part 5 (Spelling Mistakes):\", matches_part_5)\n",
    "print(\"Matches for Part 6 (Street Address Extraction):\", matches_part_6)\n",
    "print(\"Matches for Part 7 (Hexadecimal Color Codes):\", matches_part_7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disambiguated sense (fruit context): fruit\n",
      "Disambiguated sense (company context): company\n",
      "Disambiguated sense (fruit context - orchard): fruit\n",
      "Disambiguated sense (company context - tech): company\n"
     ]
    }
   ],
   "source": [
    "class SimpleWSD:\n",
    "    def __init__(self):\n",
    "        self.lexicon = {\n",
    "            'apple': ['fruit', 'company'],\n",
    "        }\n",
    "        \n",
    "        self.context_rules = {\n",
    "            'fruit': ['delicious', 'pie', 'orchard', 'tree', 'sweet', 'eat'],\n",
    "            'company': ['tech', 'profit', 'stock', 'corporation', 'business', 'CEO', 'market'],\n",
    "        }\n",
    "\n",
    "    def disambiguate(self, word, context):\n",
    "        word = word.lower()\n",
    "        context = context.lower()\n",
    "\n",
    "        if word in self.lexicon:\n",
    "            senses = self.lexicon[word]\n",
    "            \n",
    "            for sense in senses:\n",
    "                for clue in self.context_rules.get(sense, []):\n",
    "                    if clue in context:\n",
    "                        return sense\n",
    "        \n",
    "        return \"unknown\"  \n",
    "\n",
    "\n",
    "context1 = \"Apple is a delicious fruit often used in pies.\"\n",
    "context2 = \"Apple's gross profit increased by 10 percent in FY2024.\"\n",
    "context3 = \"I like to eat a fresh apple from the orchard every morning.\"\n",
    "context4 = \"Apple's new technology is transforming the market.\"\n",
    "\n",
    "wsd = SimpleWSD()\n",
    "\n",
    "print(\"Disambiguated sense (fruit context):\", wsd.disambiguate(\"apple\", context1))  \n",
    "print(\"Disambiguated sense (company context):\", wsd.disambiguate(\"apple\", context2))\n",
    "print(\"Disambiguated sense (fruit context - orchard):\", wsd.disambiguate(\"apple\", context3)) \n",
    "print(\"Disambiguated sense (company context - tech):\", wsd.disambiguate(\"apple\", context4)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: the of and a to in is as with br\n",
      "Topic 2: i the to and a this it that is of\n",
      "['negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive'] ['positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative']\n",
      "\n",
      "Sentiment Distribution:\n",
      "Positive Sentiment: 251\n",
      "Negative Sentiment: 249\n",
      "\n",
      "Sentiment Visualization:\n",
      "Positive | ***********************************************************************************************************************************************************************************************************************************************************\n",
      "Negative | *********************************************************************************************************************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('imdb_reviews.csv')\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = data.head(500)\n",
    "\n",
    "def preprocess_text(doc):\n",
    "    doc = doc.lower()  \n",
    "    doc = re.sub(r'[^a-z\\s]', '', doc) \n",
    "    return doc\n",
    "\n",
    "documents_cleaned = data['review'].apply(preprocess_text)\n",
    "\n",
    "vocab = set()\n",
    "for doc in documents_cleaned:\n",
    "    vocab.update(doc.split())\n",
    "\n",
    "vocab = sorted(vocab) \n",
    "# Create the Term-Document Matrix (TDM)\n",
    "def create_term_document_matrix(documents, vocab):\n",
    "    matrix = np.zeros((len(documents), len(vocab)))  \n",
    "    for i, doc in enumerate(documents):\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                matrix[i, vocab.index(word)] += 1  \n",
    "    return matrix\n",
    "\n",
    "X = create_term_document_matrix(documents_cleaned, vocab)\n",
    "\n",
    "# Apply Non-Negative Matrix Factorization (NMF) to extract topics\n",
    "def nmf(X, n_components=2, max_iter=500, tol=1e-4):\n",
    "    n_documents, n_terms = X.shape\n",
    "    W = np.random.rand(n_documents, n_components) \n",
    "    H = np.random.rand(n_components, n_terms) \n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        H_new = H * np.dot(W.T, X) / (np.dot(W.T, np.dot(W, H)) + 1e-10)\n",
    "        W_new = W * np.dot(X, H_new.T) / (np.dot(W, np.dot(H_new, H_new.T)) + 1e-10)\n",
    "        \n",
    "        if np.linalg.norm(W_new - W) < tol and np.linalg.norm(H_new - H) < tol:\n",
    "            break\n",
    "        \n",
    "        W, H = W_new, H_new\n",
    "        \n",
    "    return W, H\n",
    "\n",
    "W, H = nmf(X, n_components=2)\n",
    "\n",
    "def display_top_words(H, vocab, n_words=10):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_words_idx = topic.argsort()[:-n_words - 1:-1]\n",
    "        top_words = [vocab[i] for i in top_words_idx]\n",
    "        print(f\"Topic {topic_idx + 1}: {' '.join(top_words)}\")\n",
    "\n",
    "display_top_words(H, vocab)\n",
    "\n",
    "def assign_sentiment(W):\n",
    "    document_sentiments = []\n",
    "    for doc_idx in range(W.shape[0]):\n",
    "        if W[doc_idx, 0] > W[doc_idx, 1]:\n",
    "            sentiment = 'positive'\n",
    "        else:\n",
    "            sentiment = 'negative'\n",
    "        document_sentiments.append(sentiment)\n",
    "    return document_sentiments\n",
    "\n",
    "sentiments = assign_sentiment(W)\n",
    "\n",
    "\n",
    "ground_truth = data['sentiment'].tolist() \n",
    "\n",
    "print(ground_truth, sentiments)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_sentiment(sentiments):\n",
    "    positive_count = sentiments.count('positive')\n",
    "    negative_count = sentiments.count('negative')\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(f\"Positive Sentiment: {positive_count}\")\n",
    "    print(f\"Negative Sentiment: {negative_count}\")\n",
    "    \n",
    "    print(\"\\nSentiment Visualization:\")\n",
    "    print(\"Positive |\", \"*\" * positive_count)\n",
    "    print(\"Negative |\", \"*\" * negative_count)\n",
    "\n",
    "visualize_sentiment(sentiments)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: the of and a to in is as with br\n",
      "Topic 2: i the to and a this it that is of\n",
      "['negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive'] ['positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative', 'negative']\n",
      "\n",
      "Sentiment Distribution:\n",
      "Positive Sentiment: 251\n",
      "Negative Sentiment: 249\n",
      "\n",
      "Sentiment Visualization:\n",
      "Positive | ***********************************************************************************************************************************************************************************************************************************************************\n",
      "Negative | *********************************************************************************************************************************************************************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('imdb_reviews.csv')\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = data.head(500)\n",
    "\n",
    "def preprocess_text(doc):\n",
    "    doc = doc.lower()  \n",
    "    doc = re.sub(r'[^a-z\\s]', '', doc) \n",
    "    return doc\n",
    "\n",
    "documents_cleaned = data['review'].apply(preprocess_text)\n",
    "\n",
    "vocab = set()\n",
    "for doc in documents_cleaned:\n",
    "    vocab.update(doc.split())\n",
    "\n",
    "vocab = sorted(vocab) \n",
    "# Create the Term-Document Matrix (TDM)\n",
    "def create_term_document_matrix(documents, vocab):\n",
    "    matrix = np.zeros((len(documents), len(vocab)))  \n",
    "    for i, doc in enumerate(documents):\n",
    "        words = doc.split()\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                matrix[i, vocab.index(word)] += 1  \n",
    "    return matrix\n",
    "\n",
    "X = create_term_document_matrix(documents_cleaned, vocab)\n",
    "\n",
    "# Apply Non-Negative Matrix Factorization (NMF) to extract topics\n",
    "def nmf(X, n_components=2, max_iter=500, tol=1e-4):\n",
    "    n_documents, n_terms = X.shape\n",
    "    W = np.random.rand(n_documents, n_components) \n",
    "    H = np.random.rand(n_components, n_terms) \n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        H_new = H * np.dot(W.T, X) / (np.dot(W.T, np.dot(W, H)) + 1e-10)\n",
    "        W_new = W * np.dot(X, H_new.T) / (np.dot(W, np.dot(H_new, H_new.T)) + 1e-10)\n",
    "        \n",
    "        if np.linalg.norm(W_new - W) < tol and np.linalg.norm(H_new - H) < tol:\n",
    "            break\n",
    "        \n",
    "        W, H = W_new, H_new\n",
    "        \n",
    "    return W, H\n",
    "\n",
    "W, H = nmf(X, n_components=2)\n",
    "\n",
    "def display_top_words(H, vocab, n_words=10):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_words_idx = topic.argsort()[:-n_words - 1:-1]\n",
    "        top_words = [vocab[i] for i in top_words_idx]\n",
    "        print(f\"Topic {topic_idx + 1}: {' '.join(top_words)}\")\n",
    "\n",
    "display_top_words(H, vocab)\n",
    "\n",
    "def assign_sentiment(W):\n",
    "    document_sentiments = []\n",
    "    for doc_idx in range(W.shape[0]):\n",
    "        if W[doc_idx, 0] > W[doc_idx, 1]:\n",
    "            sentiment = 'positive'\n",
    "        else:\n",
    "            sentiment = 'negative'\n",
    "        document_sentiments.append(sentiment)\n",
    "    return document_sentiments\n",
    "\n",
    "sentiments = assign_sentiment(W)\n",
    "\n",
    "\n",
    "ground_truth = data['sentiment'].tolist() \n",
    "\n",
    "print(ground_truth, sentiments)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_sentiment(sentiments):\n",
    "    positive_count = sentiments.count('positive')\n",
    "    negative_count = sentiments.count('negative')\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(f\"Positive Sentiment: {positive_count}\")\n",
    "    print(f\"Negative Sentiment: {negative_count}\")\n",
    "    \n",
    "    print(\"\\nSentiment Visualization:\")\n",
    "    print(\"Positive |\", \"*\" * positive_count)\n",
    "    print(\"Negative |\", \"*\" * negative_count)\n",
    "\n",
    "visualize_sentiment(sentiments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5200\n",
      "Precision: 0.4821\n",
      "Recall: 0.5238\n",
      "True Positives: 121, False Positives: 130, False Negatives: 110, True Negatives: 139\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def accuracy(ground_truth, predicted):\n",
    "    correct = sum([1 for gt, pred in zip(ground_truth, predicted) if gt == pred])\n",
    "    total = len(ground_truth)\n",
    "    return correct / total\n",
    "\n",
    "def precision_and_recall(ground_truth, predicted):\n",
    "    # True Positives, False Positives, False Negatives\n",
    "    TP = sum([1 for gt, pred in zip(ground_truth, predicted) if gt == pred == \"positive\"])\n",
    "    FP = sum([1 for gt, pred in zip(ground_truth, predicted) if gt != pred and pred == \"positive\"])\n",
    "    FN = sum([1 for gt, pred in zip(ground_truth, predicted) if gt != pred and gt == \"positive\"])\n",
    "    TN = sum([1 for gt, pred in zip(ground_truth, predicted) if gt == pred == \"negative\"])\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "\n",
    "    return precision, recall, TP, FP, FN, TN\n",
    "\n",
    "acc = accuracy(ground_truth, sentiments)\n",
    "precision, recall, TP, FP, FN, TN = precision_and_recall(ground_truth, sentiments)\n",
    "\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"True Positives: {TP}, False Positives: {FP}, False Negatives: {FN}, True Negatives: {TN}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignmen 03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Paragraph:\n",
      "Hello! How are you today? I hope you are doing well. This is a test.\n",
      "Segmented Sentences:\n",
      " - Hello!\n",
      " - How are you today?\n",
      " - I hope you are doing well.\n",
      " - This is a test.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Paragraph:\n",
      "This is the first sentence. This is the second. Is it working? Yes, it is!\n",
      "Segmented Sentences:\n",
      " - This is the first sentence.\n",
      " - This is the second.\n",
      " - Is it working?\n",
      " - Yes, it is!\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Paragraph:\n",
      "Python is great. I love programming. Do you?\n",
      "Segmented Sentences:\n",
      " - Python is great.\n",
      " - I love programming.\n",
      " - Do you?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Paragraph:\n",
      "Hello there! How are you today? I hope you're doing well. Let's meet later in the evening.It's been a while, hasn't it? I wonder if she will come to the party. It's going to be fun.\n",
      "Segmented Sentences:\n",
      " - Hello there!\n",
      " - How are you today?\n",
      " - I hope you're doing well.\n",
      " - Let's meet later in the evening.It's been a while, hasn't it?\n",
      " - I wonder if she will come to the party.\n",
      " - It's going to be fun.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Original Paragraph:\n",
      "\n",
      "    HI\n",
      "    Hello \n",
      "    \n",
      "    My name is John Doe. I'm a software engineer. I live in New York City.\n",
      "\n",
      "    I recently graduated from Harvard University with a degree in Computer Science.\n",
      "\n",
      "    I'm currently working as a software engineer at a startup called ABC Corp.\n",
      "    \n",
      "Segmented Sentences:\n",
      " - HI\n",
      " - Hello\n",
      " - My name is John Doe.\n",
      " - I'm a software engineer.\n",
      " - I live in New York City.\n",
      " - I recently graduated from Harvard University with a degree in Computer Science.\n",
      " - I'm currently working as a software engineer at a startup called ABC Corp.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def segment_sentences(paragraph):\n",
    "    sentence_endings = r'(?<=[.!?])\\s+|\\n+' # ?<= Look a head so that . is included with sentence\n",
    "    sentences = re.split(sentence_endings, paragraph.strip())\n",
    "    return [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "text_samples = [\n",
    "    \"Hello! How are you today? I hope you are doing well. This is a test.\",\n",
    "    \"This is the first sentence. This is the second. Is it working? Yes, it is!\",\n",
    "    \"Python is great. I love programming. Do you?\",\n",
    "    \"Hello there! How are you today? I hope you're doing well. Let's meet later in the evening.It's been a while, hasn't it? I wonder if she will come to the party. It's going to be fun.\",\n",
    "    '''\n",
    "    HI\n",
    "    Hello \n",
    "    \n",
    "    My name is John Doe. I'm a software engineer. I live in New York City.\n",
    "\n",
    "    I recently graduated from Harvard University with a degree in Computer Science.\n",
    "\n",
    "    I'm currently working as a software engineer at a startup called ABC Corp.\n",
    "    '''\n",
    "]\n",
    "\n",
    "for text in text_samples:\n",
    "    print(\"Original Paragraph:\")\n",
    "    print(text)\n",
    "    print(\"Segmented Sentences:\")\n",
    "    for sentence in segment_sentences(text):\n",
    "        print(f\" - {sentence}\")\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy Accuracy: 1.0000\n",
      "Precision: 0.2692\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.4242\n",
      "\n",
      "Ground Truth Sentences:\n",
      "- London, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\n",
      "- Daniel Radcliffe as Harry Potter in 'Harry Potter and the Order of the Phoenix' To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties.\n",
      "- I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar.\n",
      "- I don't think I'll be particularly extravagant.\n",
      "- The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\n",
      "- At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film 'Hostel: Part II,' currently six places below his number one movie on the UK box office chart.\n",
      "- Details of how he'll mark his landmark birthday are under wraps.\n",
      "\n",
      "Segmented Sentences:\n",
      "- LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\n",
      "- Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties.\n",
      "- \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month.\n",
      "- \"I don't think I'll be particularly extravagant.\n",
      "- \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart.\n",
      "- Details of how he'll mark his landmark birthday are under wraps.\n",
      "- His agent and publicist had no comment on his plans.\n",
      "- \"I'll definitely have some sort of party,\" he said in an interview.\n",
      "- \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch.\n",
      "- Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground.\n",
      "- \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month.\n",
      "- \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.\n",
      "- Watch I-Reporter give her review of Potter's latest » .\n",
      "- There is life beyond Potter, however.\n",
      "- The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year.\n",
      "- He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage.\n",
      "- Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters.\n",
      "- E-mail to a friend .\n",
      "- Copyright 2007 Reuters.\n",
      "- All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "\n",
    "def calculate_fuzzy_tp_fp_fn_tn(ground_truth, segmented_sentences, threshold=80):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_negatives = 0  \n",
    "    \n",
    "    matched_ground_truth = set()  \n",
    "    for gt_sentence in ground_truth:\n",
    "        matched = False\n",
    "        for seg_sentence in segmented_sentences:\n",
    "            if fuzz.partial_ratio(gt_sentence, seg_sentence) >= threshold:\n",
    "                true_positives += 1\n",
    "                matched = True\n",
    "                matched_ground_truth.add(gt_sentence)  \n",
    "                break  \n",
    "\n",
    "        if not matched:\n",
    "            false_negatives += 1 \n",
    "\n",
    "    for seg_sentence in segmented_sentences:\n",
    "        if seg_sentence not in matched_ground_truth:\n",
    "            false_positives += 1 \n",
    "\n",
    "    total_sentences = len(ground_truth) + len(segmented_sentences)\n",
    "    true_negatives = total_sentences - (true_positives + false_positives + false_negatives)\n",
    "\n",
    "    return true_positives, false_positives, false_negatives, true_negatives\n",
    "\n",
    "def calculate_metrics(tp, fp, fn, tn):\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForTokenClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train[:1%]\")\n",
    "sample_text = dataset[0]['article']\n",
    "\n",
    "segmented_sentences = segment_sentences(sample_text)\n",
    "\n",
    "ground_truth = [\n",
    "    \"London, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him.\",\n",
    "    \"Daniel Radcliffe as Harry Potter in 'Harry Potter and the Order of the Phoenix' To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties.\",\n",
    "    \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar.\",\n",
    "    \"I don't think I'll be particularly extravagant.\",\n",
    "    \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\",\n",
    "    \"At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film 'Hostel: Part II,' currently six places below his number one movie on the UK box office chart.\",\n",
    "    \"Details of how he'll mark his landmark birthday are under wraps.\"\n",
    "]\n",
    "\n",
    "tp, fp, fn, tn = calculate_fuzzy_tp_fp_fn_tn(ground_truth, segmented_sentences, threshold=80)\n",
    "\n",
    "precision, recall, f1 = calculate_metrics(tp, fp, fn, tn)\n",
    "\n",
    "def calculate_fuzzy_accuracy(ground_truth, segmented_sentences, threshold=80):\n",
    "    correct_matches = 0\n",
    "    for gt_sentence in ground_truth:\n",
    "        for seg_sentence in segmented_sentences:\n",
    "            if fuzz.partial_ratio(gt_sentence, seg_sentence) >= threshold:\n",
    "                correct_matches += 1\n",
    "                break \n",
    "\n",
    "    accuracy = correct_matches / len(ground_truth) if len(ground_truth) > 0 else 0\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_fuzzy_accuracy(ground_truth, segmented_sentences)\n",
    "\n",
    "print(f\"Fuzzy Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nGround Truth Sentences:\")\n",
    "for sentence in ground_truth:\n",
    "    print(f\"- {sentence}\")\n",
    "\n",
    "print(\"\\nSegmented Sentences:\")\n",
    "for sentence in segmented_sentences:\n",
    "    print(f\"- {sentence}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"patient_info\": {\n",
      "            \"name\": \"John Doe\"\n",
      "        },\n",
      "        \"diagnoses\": [\n",
      "            \"hypertension\",\n",
      "            \"complaints of severe\",\n",
      "            \"headache\",\n",
      "            \"dizziness\",\n",
      "            \"Diabetes\"\n",
      "        ],\n",
      "        \"symptoms\": [],\n",
      "        \"medications\": [],\n",
      "        \"lab_results\": [\n",
      "            {\n",
      "                \"Contact\": \"555\"\n",
      "            },\n",
      "            {\n",
      "                \"DOB\": \"05/02\"\n",
      "            }\n",
      "        ],\n",
      "        \"contact\": \"555-123-4567\",\n",
      "        \"date_of_birth\": \"45-year-old\",\n",
      "        \"age\": null,\n",
      "        \"gender\": null\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "nlp_medical = spacy.load(\"en_ner_bc5cdr_md\")  \n",
    "nlp_personal = spacy.load(\"en_core_web_md\")  \n",
    "\n",
    "def extract_medical_entities(text):\n",
    "    \"\"\"\n",
    "    This function uses scispacy's NER capabilities to extract medical entities like diseases, symptoms, medications, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp_medical(text)\n",
    "    \n",
    "    entities = {\n",
    "        \"patient_info\": {},\n",
    "        \"diagnoses\": [],\n",
    "        \"symptoms\": [],\n",
    "        \"medications\": [],\n",
    "        \"lab_results\": [],\n",
    "        \"contact\": None,\n",
    "        \"date_of_birth\": None,\n",
    "        \"age\": None,\n",
    "        \"gender\": None\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        \n",
    "        if ent.label_ in [\"DISEASE\", \"MEDICAL_CONDITION\"]:\n",
    "            entities[\"diagnoses\"].append(ent.text)\n",
    "        \n",
    "        \n",
    "        if ent.label_ == \"SYMPTOM\":\n",
    "            entities[\"symptoms\"].append(ent.text)\n",
    "        \n",
    "        \n",
    "        if ent.label_ == \"MEDICATION\":\n",
    "            entities[\"medications\"].append(ent.text)\n",
    "    \n",
    "    \n",
    "    lab_results = re.findall(r\"(\\b[\\w\\s]+):\\s?(\\d+/?\\d+|\\d+)\", text)  \n",
    "    if lab_results:\n",
    "        for result in lab_results:\n",
    "            entities[\"lab_results\"].append({result[0]: result[1]})\n",
    "    \n",
    "    return entities\n",
    "\n",
    "\n",
    "def extract_personal_entities(text):\n",
    "    \"\"\"\n",
    "    This function uses the spaCy personal NER model to extract personal information like name, age, DOB, contact.\n",
    "    \"\"\"\n",
    "    \n",
    "    doc = nlp_personal(text)\n",
    "    \n",
    "    personal_info = {\n",
    "        \"patient_info\": {},\n",
    "        \"contact\": None,\n",
    "        \"date_of_birth\": None,\n",
    "        \"age\": None,\n",
    "        \"gender\": None\n",
    "    }\n",
    "    \n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        \n",
    "        if ent.label_ == \"PERSON\" and not personal_info[\"patient_info\"].get(\"name\"):\n",
    "            personal_info[\"patient_info\"][\"name\"] = ent.text\n",
    "            \n",
    "        \n",
    "        if ent.label_ == \"AGE\" and not personal_info[\"age\"]:\n",
    "            personal_info[\"age\"] = ent.text\n",
    "            \n",
    "        \n",
    "        if ent.label_ == \"DATE\" and not personal_info[\"date_of_birth\"]:\n",
    "            personal_info[\"date_of_birth\"] = ent.text\n",
    "        \n",
    "        \n",
    "        if ent.label_ == \"GENDER\" and not personal_info[\"gender\"]:\n",
    "            personal_info[\"gender\"] = ent.text\n",
    "    \n",
    "    \n",
    "    phone_match = re.search(r\"(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[\\s\\-]?\\d{3}[\\s\\-]?\\d{4})\", text)\n",
    "    if phone_match:\n",
    "        personal_info[\"contact\"] = phone_match.group(0)\n",
    "    \n",
    "    return personal_info\n",
    "\n",
    "\n",
    "def extract_data_from_clinical_notes(text):\n",
    "    \"\"\"\n",
    "    This function processes the clinical notes and extracts both medical and personal entities.\n",
    "    It returns a JSON object containing the extracted entities for both medical and personal information.\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "        text = [text]\n",
    "    \n",
    "    extracted_data = []\n",
    "    \n",
    "    for note in text:\n",
    "        medical_entities = extract_medical_entities(note)\n",
    "        personal_entities = extract_personal_entities(note)\n",
    "        \n",
    "        \n",
    "        combined_entities = {**medical_entities, **personal_entities}\n",
    "        extracted_data.append(combined_entities)\n",
    "    \n",
    "    \n",
    "    return json.dumps(extracted_data, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "John Doe, a 45-year-old male with a history of hypertension, presents with complaints of severe headache and dizziness.\n",
    "He was diagnosed with Type 2 Diabetes during a previous visit. His BP was 150/90 mmHg, and he was prescribed Metformin 500mg daily.\n",
    "Follow-up in two weeks. Contact: 555-123-4567. DOB: 05/02/1979.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "data = extract_data_from_clinical_notes(text)\n",
    "\n",
    "\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 04 (Edit Distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum Edit Distance: 1\n"
     ]
    }
   ],
   "source": [
    "def levenshtein_distance(str1, str2,substitution_cost=1,insertion_cost=1,deletion_cost=1):\n",
    "    m, n = len(str1), len(str2)\n",
    "    \n",
    "    \n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i  \n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j  \n",
    "\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]  \n",
    "            else:\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + deletion_cost,    \n",
    "                    dp[i][j - 1] + insertion_cost,    \n",
    "                    dp[i - 1][j - 1] + substitution_cost\n",
    "                )\n",
    "    \n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "str2 = \"kitten\"\n",
    "str1 = \"kittena\"\n",
    "print(\"Minimum Edit Distance:\", levenshtein_distance(str1, str2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Minimum Edit Distance: 3\n",
      "Operations:\n",
      "Substitute 'k' with 's'\n",
      "Substitute 'e' with 'i'\n",
      "Insert 'g'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def levenshtein_traceback(str1, str2):\n",
    "    m, n = len(str1), len(str2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + 1,    \n",
    "                    dp[i][j - 1] + 1,    \n",
    "                    dp[i - 1][j - 1] + 1 \n",
    "                )\n",
    "\n",
    "    \n",
    "    i, j = m, n\n",
    "    operations = []\n",
    "    \n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and str1[i - 1] == str2[j - 1]:\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and j > 0 and dp[i][j] == dp[i - 1][j - 1] + 1:\n",
    "            operations.append(f\"Substitute '{str1[i - 1]}' with '{str2[j - 1]}'\")\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif i > 0 and dp[i][j] == dp[i - 1][j] + 1:\n",
    "            operations.append(f\"Delete '{str1[i - 1]}'\")\n",
    "            i -= 1\n",
    "        elif j > 0 and dp[i][j] == dp[i][j - 1] + 1:\n",
    "            operations.append(f\"Insert '{str2[j - 1]}'\")\n",
    "            j -= 1\n",
    "\n",
    "    operations.reverse()\n",
    "    return dp[m][n], operations\n",
    "\n",
    "\n",
    "str1 = \"kitten\"\n",
    "str2 = \"sitting\"\n",
    "\n",
    "\n",
    "distance, operations = levenshtein_traceback(str1, str2)\n",
    "print(\"\\nMinimum Edit Distance:\", distance)\n",
    "print(\"Operations:\")\n",
    "for op in operations:\n",
    "    print(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 03      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit Distance Table with Backtracking Arrows:\n",
      "        s   i   t   t   i   n   g   \n",
      "     0    1←   2←   3←   4←   5←   6←   7←  \n",
      "k    1↑   1↖   2←   3←   4←   5←   6←   7←  \n",
      "i    2↑   2↑   1↖   2←   3←   4↖   5←   6←  \n",
      "t    3↑   3↑   2↑   1↖   2↖   3←   4←   5←  \n",
      "t    4↑   4↑   3↑   2↖   1↖   2←   3←   4←  \n",
      "e    5↑   5↑   4↑   3↑   2↑   2↖   3←   4←  \n",
      "n    6↑   6↑   5↑   4↑   3↑   3↑   2↖   3←  \n"
     ]
    }
   ],
   "source": [
    "def print_edit_distance_table_with_arrows(str1, str2):\n",
    "    m, n = len(str1), len(str2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    directions = [[\"\"] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "        if i > 0:\n",
    "            directions[i][0] = \"↑\"\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "        if j > 0:\n",
    "            directions[0][j] = \"←\"\n",
    "\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "                directions[i][j] = \"↖\"\n",
    "            else:\n",
    "                delete = dp[i - 1][j] + 1\n",
    "                insert = dp[i][j - 1] + 1\n",
    "                substitute = dp[i - 1][j - 1] + 1\n",
    "\n",
    "                dp[i][j] = min(delete, insert, substitute)\n",
    "\n",
    "                \n",
    "                if dp[i][j] == delete:\n",
    "                    directions[i][j] = \"↑\"\n",
    "                elif dp[i][j] == insert:\n",
    "                    directions[i][j] = \"←\"\n",
    "                else:\n",
    "                    directions[i][j] = \"↖\"\n",
    "\n",
    "    \n",
    "    print(\"Edit Distance Table with Backtracking Arrows:\")\n",
    "    print(\"    \", end=\"\")\n",
    "    for char in \" \" + str2:\n",
    "        print(f\"{char:4}\", end=\"\")\n",
    "    print()\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        if i == 0:\n",
    "            print(\"   \", end=\" \")\n",
    "        else:\n",
    "            print(f\"{str1[i - 1]:3}\", end=\" \")\n",
    "        \n",
    "        for j in range(n + 1):\n",
    "            arrow = directions[i][j]\n",
    "            print(f\"{dp[i][j]:2}{arrow:2}\", end=\" \")\n",
    "        print()\n",
    "\n",
    "print_edit_distance_table_with_arrows(str1, str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 04  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "def weighted_edit_distance(str1, str2, cost_insert=1, cost_delete=1, cost_substitute=1):\n",
    "    m, n = len(str1), len(str2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i * cost_delete\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j * cost_insert\n",
    "\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = min(\n",
    "                    dp[i - 1][j] + cost_delete,\n",
    "                    dp[i][j - 1] + cost_insert,\n",
    "                    dp[i - 1][j - 1] + cost_substitute\n",
    "                )\n",
    "\n",
    "    return dp[m][n]\n",
    "\n",
    "\n",
    "print(weighted_edit_distance(\"INTENTION\", \"EXECUTION\", cost_insert=1, cost_delete=1, cost_substitute=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soundex of 'Ahmad': 5300\n",
      "Soundex of 'Ahmed': 5300\n",
      "Do they phonetically match? True\n",
      "--------------------------------\n",
      "Soundex of 'Ayesha': 2000\n",
      "Soundex of 'Aisha': 2000\n",
      "Do they phonetically match? True\n",
      "--------------------------------\n",
      "Soundex of 'Rafiq': R120\n",
      "Soundex of 'Rafique': R120\n",
      "Do they phonetically match? True\n",
      "--------------------------------\n",
      "Soundex of 'Zain': Z500\n",
      "Soundex of 'Zayn': Z500\n",
      "Do they phonetically match? True\n",
      "--------------------------------\n",
      "Soundex of 'Naveed': N130\n",
      "Soundex of 'Naveid': N130\n",
      "Do they phonetically match? True\n",
      "--------------------------------\n",
      "Soundex of 'Subhan': S150\n",
      "Soundex of 'Suban': S150\n",
      "Do they phonetically match? True\n",
      "--------------------------------\n",
      "Soundex of 'Subhan': S150\n",
      "Soundex of 'zubhan': Z150\n",
      "Do they phonetically match? False\n",
      "--------------------------------\n",
      "Soundex of 'Rayan': R500\n",
      "Soundex of 'Rian': R500\n",
      "Do they phonetically match? True\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "def soundex(word):\n",
    "    \n",
    "    word = word.upper()\n",
    "\n",
    "    \n",
    "    first_letter = word[0]\n",
    "\n",
    "    \n",
    "    mappings = {\n",
    "        \"BFPV\": \"1\",\n",
    "        \"CGJKQSXZ\": \"2\",\n",
    "        \"DT\": \"3\",\n",
    "        \"L\": \"4\",\n",
    "        \"MN\": \"5\",\n",
    "        \"R\": \"6\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    def get_digit(character):\n",
    "        for key in mappings.keys():\n",
    "            if character in key:\n",
    "                return mappings[key]\n",
    "        return \"\"\n",
    "\n",
    "    \n",
    "    soundex_code = first_letter\n",
    "    previous_digit = None\n",
    "\n",
    "    for char in word[1:]:\n",
    "        digit = get_digit(char)\n",
    "        if digit != previous_digit:\n",
    "            soundex_code += digit\n",
    "        previous_digit = digit\n",
    "\n",
    "    \n",
    "    soundex_code = soundex_code.replace(\"A\", \"\").replace(\"E\", \"\").replace(\"I\", \"\").replace(\"O\", \"\").replace(\"U\", \"\")\n",
    "    soundex_code = soundex_code.replace(\"H\", \"\").replace(\"W\", \"\")\n",
    "\n",
    "    \n",
    "    soundex_code = (soundex_code + \"0000\")[:4]\n",
    "\n",
    "    return soundex_code\n",
    "\n",
    "def phonetic_match(str1, str2):\n",
    "    return soundex(str1) == soundex(str2)\n",
    "\n",
    "\n",
    "names = [\n",
    "    (\"Ahmad\", \"Ahmed\"),\n",
    "    (\"Ayesha\", \"Aisha\"),\n",
    "    (\"Rafiq\", \"Rafique\"),\n",
    "    (\"Zain\", \"Zayn\"),\n",
    "    (\"Naveed\", \"Naveid\"),\n",
    "    (\"Subhan\", \"Suban\"),\n",
    "    (\"Subhan\", \"zubhan\"),\n",
    "    (\"Rayan\", \"Rian\"),\n",
    "]\n",
    "\n",
    "for name1, name2 in names:\n",
    "    print(f\"Soundex of '{name1}': {soundex(name1)}\")\n",
    "    print(f\"Soundex of '{name2}': {soundex(name2)}\")\n",
    "    print(f\"Do they phonetically match? {phonetic_match(name1, name2)}\")\n",
    "    print(\"-\" * 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 04 (Sequence Alignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring Table with Backtracking Arrows:\n",
      "\n",
      "Optimal Alignment:\n",
      "AATCG\n",
      "AA-CG\n",
      "Cost: 2\n"
     ]
    }
   ],
   "source": [
    "def needleman_wunsch(str1, str2, gap_penalty=-2, match_score=1, mismatch_penalty=-1):\n",
    "    m, n = len(str1), len(str2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    directions = [[\"\"] * (n + 1) for _ in range(m + 1)]\n",
    "\n",
    "    \n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i * gap_penalty\n",
    "        directions[i][0] = \"↑\"\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j * gap_penalty\n",
    "        directions[0][j] = \"←\"\n",
    "    \n",
    "    directions[0][0] = \" \"\n",
    "\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            match = dp[i - 1][j - 1] + (match_score if str1[i - 1] == str2[j - 1] else mismatch_penalty)\n",
    "            delete = dp[i - 1][j] + gap_penalty\n",
    "            insert = dp[i][j - 1] + gap_penalty\n",
    "\n",
    "            \n",
    "            if match >= delete and match >= insert:\n",
    "                dp[i][j] = match\n",
    "                directions[i][j] = \"↖\"\n",
    "            elif delete > match and delete >= insert:\n",
    "                dp[i][j] = delete\n",
    "                directions[i][j] = \"↑\"\n",
    "            else:\n",
    "                dp[i][j] = insert\n",
    "                directions[i][j] = \"←\"\n",
    "\n",
    "    \n",
    "    print(\"Scoring Table with Backtracking Arrows:\")\n",
    "\n",
    "    \n",
    "    i, j = m, n\n",
    "    align1, align2 = \"\", \"\"\n",
    "\n",
    "    while i > 0 and j > 0:\n",
    "        if directions[i][j] == \"↖\":\n",
    "            align1 += str1[i - 1]\n",
    "            align2 += str2[j - 1]\n",
    "            i -= 1\n",
    "            j -= 1\n",
    "        elif directions[i][j] == \"↑\":\n",
    "            align1 += str1[i - 1]\n",
    "            align2 += \"-\"\n",
    "            i -= 1\n",
    "        elif directions[i][j] == \"←\":\n",
    "            align1 += \"-\"\n",
    "            align2 += str2[j - 1]\n",
    "            j -= 1\n",
    "\n",
    "    \n",
    "    while i > 0:\n",
    "        align1 += str1[i - 1]\n",
    "        align2 += \"-\"\n",
    "        i -= 1\n",
    "    while j > 0:\n",
    "        align1 += \"-\"\n",
    "        align2 += str2[j - 1]\n",
    "        j -= 1\n",
    "\n",
    "    \n",
    "    print(\"\\nOptimal Alignment:\")\n",
    "    print(align1[::-1])\n",
    "    print(align2[::-1])\n",
    "    return dp[m][n],dp,directions\n",
    "\n",
    "\n",
    "str1=\"AATCG\"\n",
    "str2=\"AACG\"\n",
    "cost,dp,directions=needleman_wunsch(str1,str2)\n",
    "print(\"Cost:\",cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A     A     C     G     \n",
      "     0    -2←   -4←   -6←   -8←  \n",
      "A   -2↑    1↖   -1↖   -3←   -5←  \n",
      "A   -4↑   -1↖    2↖    0←   -2←  \n",
      "T   -6↑   -3↑    0↑    1↖   -1↖  \n",
      "C   -8↑   -5↑   -2↑    1↖    0↖  \n",
      "G  -10↑   -7↑   -4↑   -1↑    2↖  \n"
     ]
    }
   ],
   "source": [
    "def print_table_with_arrows(dp, directions, str1, str2):\n",
    "    \n",
    "    print(\"    \", end=\"\")\n",
    "    for char in \" \" + str2:\n",
    "        print(f\"{char:6}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "    for i, row in enumerate(dp):\n",
    "        if i == 0:\n",
    "            print(\" \", end=\"  \")\n",
    "        else:\n",
    "            print(str1[i - 1], end=\"  \")\n",
    "        for j, val in enumerate(row):\n",
    "            arrow = directions[i][j]\n",
    "            print(f\"{val:3}{arrow:3}\", end=\"\")\n",
    "        print()\n",
    "        \n",
    "print_table_with_arrows(dp, directions, str1, str2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
